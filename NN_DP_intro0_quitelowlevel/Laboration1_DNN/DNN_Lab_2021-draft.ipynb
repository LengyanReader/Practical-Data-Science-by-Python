{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks Laboration\n",
    "\n",
    "Data used in this laboration are from the Kitsune Network Attack Dataset, https://archive.ics.uci.edu/ml/datasets/Kitsune+Network+Attack+Dataset . We will focus on the 'Mirai' part of the dataset. Your task is to make a DNN that can classify if each attack is benign or malicious. The dataset has 116 covariates, but to make it a bit more difficult we will remove the first 24 covariates.\n",
    "\n",
    "You need to answer all questions in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Get the data\n",
    "\n",
    "Skip this part if you load stored numpy arrays (Mirai*.npy) (which is recommended)\n",
    "\n",
    "Use `wget` in the terminal of your cloud machine (in the same directory as where you have saved this notebook) to download the data, i.e.\n",
    "\n",
    "wget https://archive.ics.uci.edu/ml/machine-learning-databases/00516/mirai/Mirai_dataset.csv.gz\n",
    "\n",
    "wget https://archive.ics.uci.edu/ml/machine-learning-databases/00516/mirai/Mirai_labels.csv.gz\n",
    "\n",
    "Then unpack the files using `gunzip` in the terminal, i.e.\n",
    "\n",
    "gunzip Mirai_dataset.csv.gz\n",
    "\n",
    "gunzip Mirai_labels.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Get a graphics card\n",
    "\n",
    "Skip this part if you run on the CPU\n",
    "\n",
    "Lets make sure that our script can see the graphics card that will be used. The graphics cards will perform all the time consuming calculations in every training iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "# Ignore FutureWarning from numpy\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    " \n",
    "# The GPU id to use, usually either \"0\" or \"1\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";\n",
    "\n",
    "# Allow growth of GPU memory, otherwise it will always look like all the memory is being used\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Hardware\n",
    "\n",
    "Skip questions 1, 2, 3, 5, 6 if you use your own computer.\n",
    "\n",
    "In deep learning, the computer hardware is very important. You should always know what kind of hardware you are working on.\n",
    "\n",
    "Question 1: What graphics card is available in the cloud machine? Run 'nvidia-smi' in the terminal. \n",
    "\n",
    "Question 2: Google the name of the graphics card, how many CUDA cores does it have?\n",
    "\n",
    "Question 3: How much memory does the graphics card have?\n",
    "\n",
    "Question 4: What is stored in the GPU memory while training a DNN ?\n",
    "\n",
    "Question 5: What CPU is available in the cloud machine? How many cores does it have? Run 'lscpu' in the terminal.\n",
    "\n",
    "Question 6: How much CPU memory (RAM) is available in the cloud machine? Run 'free -g' in the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Load the data\n",
    "\n",
    "To make this step easier, directly load the data from saved numpy arrays (.npy) (recommended)\n",
    "\n",
    "\n",
    "Load the dataset from the csv files, it will take some time since it is almost 1.4 GB. \n",
    "\n",
    "We will use the function `genfromtxt` to load the data.\n",
    "\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html\n",
    "\n",
    "Load the data from csv files the first time, then save the data as numpy files for faster loading the next time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The covariates have size (764137, 92).\n",
      "The labels have size (764137,).\n",
      "the number of examples of each class:{0.0: 121621, 1.0: 642516}\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "\n",
    "# Load data from numpy arrays\n",
    "X = np.load('Mirai_data.npy')\n",
    "Y = np.load('Mirai_labels.npy')\n",
    "\n",
    "\n",
    "\n",
    "# Remove the first 24 covariates (columns)\n",
    "X=X[:,24:]\n",
    "\n",
    "print('The covariates have size {}.'.format(X.shape))\n",
    "print('The labels have size {}.'.format(Y.shape))\n",
    "\n",
    "# Print the number of examples of each class\n",
    "label=np.unique(Y)\n",
    "#print(label)\n",
    "dic_num_cls={}\n",
    "for i in label:\n",
    "    dic_num_cls[i]=np.count_nonzero(Y==i)\n",
    "print('the number of examples of each class:{}'.format(dic_num_cls) )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: How good is a naive classifier?\n",
    "\n",
    "Question 7: Given the number of examples from each class, how high classification performance can a naive classifier obtain? The naive classifier will assume that all examples belong to one class. Note: you do not need to make a naive classifier, this is a theoretical question, just to understand how good performance we can obtain by guessing that all examples belong to one class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It is common to have NaNs in the data, lets check for it. Hint: np.isnan()\n",
    "np.unique(np.isnan(X))\n",
    "\n",
    "# Print the number of NaNs (not a number) in the labels\n",
    "np.where(np.isnan(X).any(axis=1)==True)[0]\n",
    "len(np.where(np.isnan(X).any(axis=1)==True)[0])\n",
    "\n",
    "# Print the number of NaNs in the covariates\n",
    "\n",
    "np.count_nonzero(~np.isnan(X)) # 764137*92\n",
    "np.count_nonzero(np.isnan(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Preprocessing\n",
    "\n",
    "Lets do some simple preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "int32\n",
      "[-3.19451533e-18 -6.32970181e-14  1.19926356e-13  4.56743018e-15\n",
      "  4.10210037e-14  1.46130975e-13  5.85246484e-16 -1.69734859e-14\n",
      " -3.36915700e-13  1.28688437e-12 -2.69360995e-12 -1.10733213e-13\n",
      " -1.22392702e-13 -1.70649630e-13 -1.02461166e-14  2.50701280e-12\n",
      "  1.47553162e-12  1.08446837e-12 -1.04981959e-13  6.83458762e-14\n",
      " -1.03373555e-13  5.98825773e-14 -1.02025960e-12 -1.68983055e-12\n",
      " -1.79101143e-12 -1.31828514e-13  4.42580403e-13  6.14635580e-13\n",
      "  5.78048199e-14 -4.92623328e-13 -2.54513072e-12  1.86544900e-13\n",
      " -1.53444593e-13  1.68079591e-12  9.30041709e-13  1.50738177e-13\n",
      " -1.15688852e-12 -3.62610361e-13 -1.71390937e-12 -2.09264067e-13\n",
      "  1.07161976e-12 -1.45236885e-12 -1.69724579e-14 -1.64918984e-16\n",
      " -5.13444996e-14 -1.02171349e-14 -1.74685907e-15  1.34264921e-13\n",
      "  5.98801969e-14  1.48745574e-17 -4.25442340e-13  5.78079594e-14\n",
      "  1.25638129e-15  1.69449684e-13  1.50725881e-13  2.14439542e-14\n",
      "  3.65457183e-14  1.17260451e-13 -8.82752870e-13 -6.34816648e-13\n",
      " -1.62109649e-12  2.63270303e-13 -7.57215123e-15 -2.89395002e-14\n",
      " -3.90180996e-13 -1.53167085e-12 -9.57913621e-13  2.47411065e-13\n",
      "  2.44200541e-13 -6.73050928e-15  1.07502596e-13  2.58222203e-13\n",
      " -1.87714601e-13 -1.19882476e-12 -2.17154862e-12  5.48444735e-14\n",
      "  5.46183481e-15  3.71315442e-14  1.47576646e-13 -1.62639245e-12\n",
      " -1.23986972e-13 -1.71744315e-12  5.29956657e-13 -3.21442452e-14\n",
      " -4.59767392e-14  3.56347870e-13 -1.48544246e-12 -1.26642728e-13\n",
      "  1.52633871e-13  9.58048710e-14  4.34603426e-14 -4.07615740e-14]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Convert covariates to floats\n",
    "X = X.astype('float64') \n",
    "print(X.dtype)\n",
    "# Convert labels to integers\n",
    "Y = Y.astype('int')\n",
    "print(Y.dtype)\n",
    "\n",
    "# Remove mean of each covariate (column)\n",
    "col_means = np.mean(X, axis=0)\n",
    "col_means.shape\n",
    "#col_means\n",
    "len_col=X.shape[1]\n",
    "for i in range(len_col):\n",
    "    X[:,i]=X[:,i]-col_means[i]\n",
    "    \n",
    "# Divide each covariate (column) by its standard deviation\n",
    "col_std = np.std(X, axis=0)\n",
    "for i in range(len_col):\n",
    "    X[:,i]=X[:,i]/col_std[i]\n",
    "# Check that mean is 0 and standard deviation is 1 for all covariates, by printing mean and std\n",
    "col_means = np.mean(X, axis=0)\n",
    "col_std = np.std(X, axis=0)\n",
    "print(col_means)\n",
    "print(col_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Split the dataset\n",
    "\n",
    "Use the first 70% of the dataset for training, leave the other 30% for validation and test, call the variables\n",
    "\n",
    "Xtrain (70%)\n",
    "\n",
    "Xtemp  (30%)\n",
    "\n",
    "Ytrain (70%)\n",
    "\n",
    "Ytemp  (30%)\n",
    "\n",
    "We use a function from scikit learn.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(42)\n",
    "# Xrow = np.arange(X.shape[0])\n",
    "# np.random.shuffle(Xrow)\n",
    "# len_pct30=np.ceil(X.shape[0]*0.3).astype('int')\n",
    "# Xtrain=X[:len_pct30,]\n",
    "# Ytrain=Y[:len_pct30,]\n",
    "# Xtemp=X[len_pct30:,]\n",
    "# Ytemp=Y[len_pct30:,]\n",
    "#from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain has size (534895, 92).\n",
      "Ytrain has size (534895,).\n",
      "Xtemp has size (229242, 92).\n",
      "Ytemp has size (229242,).\n",
      "85213\n",
      "449682\n",
      "36408\n",
      "192834\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Your code\n",
    "Xtrain, Xtemp, Ytrain, Ytemp = train_test_split( X, Y, test_size=0.30, random_state=12345)\n",
    "\n",
    "print('Xtrain has size {}.'.format(Xtrain.shape))\n",
    "print('Ytrain has size {}.'.format(Ytrain.shape))\n",
    "\n",
    "\n",
    "print('Xtemp has size {}.'.format(Xtemp.shape))\n",
    "print('Ytemp has size {}.'.format(Ytemp.shape))\n",
    "\n",
    "# Print the number of examples of each class, for the training data and the remaining 30%\n",
    "print(np.count_nonzero(Ytrain==0))\n",
    "print(np.count_nonzero(Ytrain==1))\n",
    "print(np.count_nonzero(Ytemp==0))\n",
    "print(np.count_nonzero(Ytemp==1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Part 8: Split non-training data data into validation and test\n",
    "Now split your non-training data (Xtemp, Ytemp) into 50% validation (Xval, Yval) and 50% testing (Xtest, Ytest), we use a function from scikit learn. \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "Do all variables (Xtrain,Ytrain), (Xval,Yval), (Xtest,Ytest) have the shape that you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation and test data have size (114621, 92), (114621, 92), (114621,) and (114621,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xval, Xtest, Yval, Ytest = train_test_split( Xtemp, Ytemp, test_size=0.5, random_state=1234567)\n",
    "\n",
    "print('The validation and test data have size {}, {}, {} and {}'.format(Xval.shape, Xtest.shape, Yval.shape, Ytest.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 9: DNN classification\n",
    "\n",
    "Finish this code to create a first version of the classifier using a DNN. Start with a simple network with 2 dense layers (with 20 nodes each), using sigmoid activation functions. The final dense layer should have a single node and a sigmoid activation function. We start with the SGD optimizer.\n",
    "\n",
    "For different parts of this notebook you need to go back here, add more things, and re-run this cell to re-define the build function.\n",
    "\n",
    "Relevant functions are\n",
    "\n",
    "`model.add()`, adds a layer to the network\n",
    "\n",
    "`Dense()`, a dense network layer\n",
    "\n",
    "`model.compile()`, compile the model, add \" metrics=['accuracy'] \" to print the classification accuracy during the training\n",
    "\n",
    "See https://keras.io/layers/core/ for information on how the `Dense()` function works\n",
    "\n",
    "Import a relevant cost / loss function for binary classification from keras.losses (https://keras.io/losses/)\n",
    "\n",
    "See the following links for how to compile, train and evaluate the model\n",
    "\n",
    "https://keras.io/api/models/model_training_apis/#compile-method\n",
    "\n",
    "https://keras.io/api/models/model_training_apis/#fit-method\n",
    "\n",
    "https://keras.io/api/models/model_training_apis/#evaluate-method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import SGD\n",
    "#from keras.losses import sparse_categorical_accuracy\n",
    "\n",
    "# Set seed from random number generator, for better comparisons\n",
    "from numpy.random import seed\n",
    "seed(123)\n",
    "\n",
    "def build_DNN(input_shape, n_layers, n_nodes, act_fun='sigmoid', optimizer='sgd', learning_rate=0.01, \n",
    "              use_bn=False, use_dropout=False, use_custom_dropout=False):\n",
    "    \n",
    "    # Setup optimizer, depending on input parameter string, put it to the last\n",
    "    opt = SGD(learning_rate=0.01)\n",
    "    # Setup a sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add layers to the model, using the input parameters of the build_DNN function\n",
    "    model.add(Input(shape=(input_shape,)  ))\n",
    "    \n",
    "    # Add first layer, requires input shape\n",
    "    model.add(Dense(n_nodes, activation=act_fun))\n",
    "    \n",
    "    \n",
    "    # Add remaining layers, do not require input shape\n",
    "    for i in range(n_layers-1):    ###1\n",
    "        model.add(Dense(n_nodes, activation=act_fun))     \n",
    "           \n",
    "       \n",
    "    # Final layer\n",
    "    model.add(Dense(1,activation='sigmoid'))  \n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer=opt,loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model0=build_DNN(input_shape, n_layers, n_nodes, act_fun='sigmoid', optimizer='sgd', learning_rate=0.01, \n",
    "#               use_bn=False, use_dropout=False, use_custom_dropout=False)\n",
    "# model0.summary()\n",
    "\n",
    "# model0(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define a help function for plotting the training results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_results(history):\n",
    "    \n",
    "    val_loss = history.history['val_loss']\n",
    "    acc = history.history['accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    \n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(loss)\n",
    "    plt.plot(val_loss)\n",
    "    plt.legend(['Training','Validation'])\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.plot(acc)\n",
    "    plt.plot(val_acc)\n",
    "    plt.legend(['Training','Validation'])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 10: Train the DNN\n",
    "\n",
    "Time to train the DNN, we start simple with 2 layers with 20 nodes each, learning rate 0.1.\n",
    "\n",
    "Relevant functions\n",
    "\n",
    "`build_DNN`, the function we defined in Part 9, call it with the parameters you want to use\n",
    "\n",
    "`model.fit()`, train the model with some training data\n",
    "\n",
    "`model.evaluate()`, apply the trained model to some test data\n",
    "\n",
    "See the following links for how to train and evaluate the model\n",
    "\n",
    "https://keras.io/api/models/model_training_apis/#fit-method\n",
    "\n",
    "https://keras.io/api/models/model_training_apis/#evaluate-method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers, 20 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_104 (Dense)            (None, 20)                1860      \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "54/54 [==============================] - 1s 12ms/step - loss: 0.5577 - accuracy: 0.8417 - val_loss: 0.4835 - val_accuracy: 0.8402\n",
      "Epoch 2/20\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.4568 - accuracy: 0.8407 - val_loss: 0.4389 - val_accuracy: 0.8402\n",
      "Epoch 3/20\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4296 - accuracy: 0.8407 - val_loss: 0.4233 - val_accuracy: 0.8402\n",
      "Epoch 4/20\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4183 - accuracy: 0.8407 - val_loss: 0.4150 - val_accuracy: 0.8402\n",
      "Epoch 5/20\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4111 - accuracy: 0.8407 - val_loss: 0.4088 - val_accuracy: 0.8402\n",
      "Epoch 6/20\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4052 - accuracy: 0.8407 - val_loss: 0.4032 - val_accuracy: 0.8402\n",
      "Epoch 7/20\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.3997 - accuracy: 0.8407 - val_loss: 0.3977 - val_accuracy: 0.8402\n",
      "Epoch 8/20\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.3942 - accuracy: 0.8407 - val_loss: 0.3922 - val_accuracy: 0.8402\n",
      "Epoch 9/20\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.3887 - accuracy: 0.8407 - val_loss: 0.3867 - val_accuracy: 0.8402\n",
      "Epoch 10/20\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.3831 - accuracy: 0.8407 - val_loss: 0.3810 - val_accuracy: 0.8402\n",
      "Epoch 11/20\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.3774 - accuracy: 0.8407 - val_loss: 0.3753 - val_accuracy: 0.8402\n",
      "Epoch 12/20\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.3716 - accuracy: 0.8407 - val_loss: 0.3695 - val_accuracy: 0.8402\n",
      "Epoch 13/20\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.3657 - accuracy: 0.8407 - val_loss: 0.3636 - val_accuracy: 0.8402\n",
      "Epoch 14/20\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.3598 - accuracy: 0.8407 - val_loss: 0.3577 - val_accuracy: 0.8402\n",
      "Epoch 15/20\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 0.3538 - accuracy: 0.8407 - val_loss: 0.3516 - val_accuracy: 0.8402\n",
      "Epoch 16/20\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.3478 - accuracy: 0.8407 - val_loss: 0.3456 - val_accuracy: 0.8402\n",
      "Epoch 17/20\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.3417 - accuracy: 0.8407 - val_loss: 0.3395 - val_accuracy: 0.8402\n",
      "Epoch 18/20\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.3356 - accuracy: 0.8407 - val_loss: 0.3334 - val_accuracy: 0.8402\n",
      "Epoch 19/20\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.3295 - accuracy: 0.8407 - val_loss: 0.3273 - val_accuracy: 0.8402\n",
      "Epoch 20/20\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.3234 - accuracy: 0.8407 - val_loss: 0.3212 - val_accuracy: 0.8402\n"
     ]
    }
   ],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "\n",
    "\n",
    "input_shape = X.shape[1]\n",
    "n_layers = 2\n",
    "n_nodes = 20\n",
    "\n",
    "# Build the model\n",
    "model1 = build_DNN(input_shape, n_layers, n_nodes, act_fun='sigmoid', optimizer='sgd', learning_rate=0.01, \n",
    "              use_bn=False, use_dropout=False, use_custom_dropout=False)\n",
    "\n",
    "model1.summary()\n",
    "history1=model1.fit(Xtrain, Ytrain, epochs=epochs,batch_size=batch_size,validation_data=(Xval,Yval))\n",
    "\n",
    "# Train the model, provide training data and validation data\n",
    "#history1 = plot_results(fit1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_38\"\n",
      "___________________________________________________________________________________________________________________________\n",
      "Layer (type)                                           Output Shape                                     Param #            \n",
      "===========================================================================================================================\n",
      "dense_104 (Dense)                                      (None, 20)                                       1860               \n",
      "___________________________________________________________________________________________________________________________\n",
      "dense_105 (Dense)                                      (None, 20)                                       420                \n",
      "___________________________________________________________________________________________________________________________\n",
      "dense_106 (Dense)                                      (None, 1)                                        21                 \n",
      "===========================================================================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "___________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2301"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/62913689/why-do-i-get-6-parameters-in-keras-in-simple-2-output-2-input-network\n",
    "model1.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.0682249 , -0.03362398, -0.1000131 , ..., -0.01918482,\n",
       "          0.04564133,  0.12123282],\n",
       "        [ 0.09893011,  0.01232966, -0.16255042, ..., -0.16478956,\n",
       "          0.06425702,  0.30572045],\n",
       "        [ 0.13039456, -0.07458293, -0.0123549 , ..., -0.14136086,\n",
       "         -0.10193576,  0.03285349],\n",
       "        ...,\n",
       "        [ 0.09498128,  0.17234409,  0.20491739, ..., -0.18158245,\n",
       "         -0.05575847, -0.00646567],\n",
       "        [ 0.22861981, -0.04352651,  0.12663724, ...,  0.1736004 ,\n",
       "          0.09583703,  0.03763353],\n",
       "        [-0.09919674,  0.12131887,  0.19883437, ..., -0.01723318,\n",
       "         -0.1690433 , -0.04063721]], dtype=float32),\n",
       " array([ 0.0060769 , -0.00141334, -0.00383063,  0.01561234, -0.00798404,\n",
       "         0.01475803, -0.01541509,  0.00983956,  0.01038972,  0.00575143,\n",
       "         0.00685712,  0.00512085, -0.00859069, -0.00067068, -0.00098216,\n",
       "        -0.00136154,  0.02691223, -0.00245178, -0.00338106,  0.02769405],\n",
       "       dtype=float32),\n",
       " array([[-0.04470812,  0.11102189, -0.31537464,  0.04076394,  0.18883064,\n",
       "         -0.25809422,  0.38610646, -0.23246999, -0.2986944 ,  0.33009714,\n",
       "          0.23011129, -0.32197344,  0.14902627,  0.03041846,  0.07153605,\n",
       "          0.40199745, -0.15944082, -0.21347135,  0.07026161,  0.11594157],\n",
       "        [-0.18017535, -0.05490465,  0.2630832 ,  0.24674857, -0.08811634,\n",
       "         -0.1437155 , -0.10488375, -0.00709413, -0.17378584,  0.13628271,\n",
       "          0.2378383 ,  0.33985937,  0.05524841, -0.30271518, -0.06877407,\n",
       "          0.36286566,  0.0519911 ,  0.28159338, -0.24202003,  0.3207767 ],\n",
       "        [ 0.2754098 , -0.21498694,  0.00660639,  0.31691793,  0.09613886,\n",
       "          0.38042754, -0.05508842, -0.11273652, -0.3793095 ,  0.31540406,\n",
       "          0.22799411, -0.32697687,  0.03829121,  0.09690186, -0.288139  ,\n",
       "         -0.1374481 , -0.12673575,  0.19656788,  0.32891473, -0.35283   ],\n",
       "        [-0.27391213,  0.2627426 , -0.17974994, -0.10066456,  0.20841974,\n",
       "         -0.05171027,  0.18430345,  0.07002515,  0.3429339 , -0.2639102 ,\n",
       "         -0.24316832, -0.13195819,  0.05853963, -0.03229032, -0.36188656,\n",
       "          0.19111909, -0.3149    ,  0.17220812,  0.28847918, -0.09773199],\n",
       "        [-0.24485779, -0.20790993,  0.34648582, -0.05361496, -0.18164141,\n",
       "         -0.00757144, -0.11880923,  0.19208825,  0.28231025,  0.3719786 ,\n",
       "         -0.27715534, -0.01794127, -0.19903524, -0.35507563, -0.14128679,\n",
       "         -0.19361544, -0.3330838 ,  0.29432532,  0.05271616,  0.14362016],\n",
       "        [-0.36952934, -0.01201709, -0.27767938, -0.34649664,  0.3983368 ,\n",
       "         -0.07696585, -0.03912671,  0.01926999,  0.4206411 , -0.00503915,\n",
       "         -0.08078317,  0.16272594,  0.12189425, -0.0442061 , -0.3275756 ,\n",
       "         -0.0081597 , -0.00308887, -0.20691566,  0.34758326,  0.12466008],\n",
       "        [-0.11179766, -0.01060626,  0.36950803,  0.02521021,  0.01788297,\n",
       "          0.3950025 ,  0.03929602, -0.36503622, -0.18319935, -0.11027095,\n",
       "         -0.16549404, -0.3744308 ,  0.24876347,  0.20007609,  0.1409866 ,\n",
       "         -0.04541332,  0.10386707, -0.2805915 , -0.26966375,  0.05375995],\n",
       "        [ 0.2051722 , -0.08034996, -0.33763275, -0.3346841 , -0.00536018,\n",
       "         -0.0498864 , -0.16918084,  0.00230437, -0.23794626,  0.01074127,\n",
       "          0.347289  , -0.29441643, -0.39134014, -0.3852467 , -0.29767933,\n",
       "         -0.11244424, -0.34818736, -0.1682145 ,  0.3181892 , -0.3566284 ],\n",
       "        [-0.29377484,  0.3432975 , -0.2708291 ,  0.06047233,  0.37392336,\n",
       "         -0.05940996,  0.06579526,  0.12205926, -0.30782524, -0.07492335,\n",
       "          0.40373796, -0.32306337,  0.04025061,  0.01817762, -0.29581556,\n",
       "          0.11866929,  0.29072872,  0.3069424 , -0.27602488,  0.30420825],\n",
       "        [-0.20579042,  0.00215105,  0.29743302, -0.11319304,  0.0570231 ,\n",
       "          0.18494369, -0.04853233, -0.368713  ,  0.15477523,  0.24222682,\n",
       "          0.28815904, -0.30244306,  0.19967085,  0.40839252,  0.02474101,\n",
       "         -0.0711008 ,  0.06965835,  0.27683946, -0.3211493 , -0.29580724],\n",
       "        [ 0.0402234 , -0.04742021,  0.1030132 ,  0.21354537, -0.1787646 ,\n",
       "         -0.37619957,  0.1037169 , -0.10852569,  0.34180897, -0.03508008,\n",
       "          0.02326351, -0.26884317, -0.2075574 , -0.23433922,  0.36941776,\n",
       "         -0.12626047, -0.05873003, -0.01879012, -0.02140129, -0.33372492],\n",
       "        [ 0.13953717, -0.2619102 , -0.16389585, -0.3177384 , -0.35534543,\n",
       "         -0.14477153, -0.10670131, -0.09242367,  0.12413224, -0.08535416,\n",
       "          0.27198824, -0.10087582, -0.27337682,  0.39176482,  0.06677052,\n",
       "         -0.36309114,  0.02143783,  0.02285435,  0.21467395,  0.3065198 ],\n",
       "        [-0.24368942, -0.44232762, -0.02464225, -0.30199102, -0.21868934,\n",
       "          0.3412324 , -0.32961503, -0.33008987, -0.04373147,  0.0248103 ,\n",
       "         -0.0828629 , -0.04216814, -0.2997724 ,  0.3493364 , -0.17445855,\n",
       "          0.04402139, -0.05373345,  0.0652505 , -0.42562625, -0.04334777],\n",
       "        [ 0.33380258,  0.20750855,  0.0315609 , -0.10515022,  0.09497108,\n",
       "         -0.14432423, -0.3398734 ,  0.29093218, -0.23675163, -0.01806536,\n",
       "          0.08055454,  0.18289098, -0.29333636,  0.33177418, -0.15747434,\n",
       "          0.2992209 , -0.08761183,  0.10399384,  0.09095304,  0.06045932],\n",
       "        [-0.22114664,  0.12061104,  0.24617666, -0.1914712 , -0.27013272,\n",
       "         -0.07156932, -0.10613393,  0.29396394,  0.06631406, -0.31408912,\n",
       "          0.03713445, -0.37104288,  0.23126344,  0.1191356 ,  0.19715936,\n",
       "         -0.06902955,  0.38076016, -0.10169435,  0.1593414 , -0.17156406],\n",
       "        [-0.28431833,  0.1399309 ,  0.16855654,  0.16198756,  0.26301163,\n",
       "          0.06517965, -0.3310367 ,  0.20566669, -0.07962454,  0.27990854,\n",
       "          0.27858183,  0.32798293,  0.2658394 ,  0.00794888, -0.11907165,\n",
       "          0.24257784,  0.07288603, -0.15296522, -0.3098932 , -0.26992106],\n",
       "        [ 0.06967141,  0.43268085, -0.37300432,  0.035564  ,  0.37714064,\n",
       "         -0.01979063,  0.19271886, -0.34597796, -0.14377104, -0.19984818,\n",
       "          0.32272464, -0.26391938,  0.28628394, -0.3674977 , -0.1719869 ,\n",
       "          0.01486245,  0.28269345, -0.11786144,  0.05535506, -0.19715215],\n",
       "        [ 0.09583118,  0.2342344 ,  0.04893305, -0.08394405,  0.39964497,\n",
       "         -0.03222514,  0.2134914 ,  0.07466648,  0.41284078,  0.08573329,\n",
       "         -0.28301   , -0.36395147,  0.17815438, -0.2806278 ,  0.23048411,\n",
       "         -0.16879866, -0.1920271 ,  0.16350856, -0.32061306,  0.18158029],\n",
       "        [-0.06231366, -0.08928837,  0.23824504,  0.22361983, -0.15753993,\n",
       "         -0.17114364,  0.07969844, -0.35765085, -0.39868656,  0.07499386,\n",
       "          0.05281928, -0.24839394, -0.22372034,  0.04118187, -0.00390288,\n",
       "         -0.06635368, -0.22987553, -0.34926414,  0.21340951,  0.19116738],\n",
       "        [-0.0489406 ,  0.47034085,  0.18972471, -0.00431021,  0.03215018,\n",
       "         -0.28768876, -0.23431347,  0.3241761 ,  0.3220043 , -0.07194222,\n",
       "          0.14643672,  0.14306627, -0.3080294 , -0.3697636 , -0.10352738,\n",
       "          0.37453076, -0.23357093,  0.36711407,  0.3040263 , -0.3904952 ]],\n",
       "       dtype=float32),\n",
       " array([-0.00934832,  0.0039847 , -0.02409629, -0.01441242,  0.00596011,\n",
       "        -0.01706946,  0.01309798, -0.02809628,  0.02312962, -0.02225002,\n",
       "         0.04090216, -0.00231925, -0.02349489, -0.01580777, -0.00291452,\n",
       "         0.01490595,  0.0150449 ,  0.01646731,  0.01790022, -0.01198319],\n",
       "       dtype=float32),\n",
       " array([[-0.16351967],\n",
       "        [ 0.7511275 ],\n",
       "        [-0.39967188],\n",
       "        [-0.19678007],\n",
       "        [ 0.75354785],\n",
       "        [-0.48971277],\n",
       "        [ 0.28814253],\n",
       "        [-0.17474642],\n",
       "        [ 0.527898  ],\n",
       "        [-0.3378688 ],\n",
       "        [ 0.6378482 ],\n",
       "        [ 0.00462476],\n",
       "        [-0.26409096],\n",
       "        [-0.50633454],\n",
       "        [-0.09886289],\n",
       "        [ 0.56787175],\n",
       "        [ 0.32722992],\n",
       "        [ 0.39448217],\n",
       "        [ 0.5434447 ],\n",
       "        [-0.20654304]], dtype=float32),\n",
       " array([0.2210936], dtype=float32)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.layers[0].get_weights()\n",
    "model1.layers[1].get_weights()\n",
    "model1.layers[2].get_weights()\n",
    "#model1.layers[3].get_weights()\n",
    "model1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 0.3183 - accuracy: 0.8422\n",
      "Test loss: 0.3183\n",
      "Test accuracy: 0.8422\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "score=model1.evaluate(Xtest,  Ytest, batch_size=batch_size)\n",
    "\n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the history from the training run\n",
    "plot_results(history1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 11: More questions\n",
    "\n",
    "Question 8: What happens if you add several Dense layers without specifying the activation function?\n",
    "\n",
    "Question 9: How are the weights in each dense layer initialized as default? How are the bias weights initialized?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 12: Balancing the classes\n",
    "\n",
    "This dataset is rather unbalanced, we need to define class weights so that the training pays more attention to the class with fewer samples. We use a function in scikit learn\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Calculate class weights\n",
    "\n",
    "\n",
    "# Print the class weights\n",
    "\n",
    "# Keras wants the weights in this form, uncomment and change value1 and value2 to your weights, \n",
    "# or get them from the array that is returned from class_weight\n",
    "\n",
    "#class_weights = {0: value1,\n",
    "#                 1: value2}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers, 20 nodes, class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = ?\n",
    "\n",
    "# Build and train model\n",
    "model2 = \n",
    "\n",
    "history2 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = \n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 13: More questions\n",
    "\n",
    "Skip questions 11 and 12 if you run on the CPU\n",
    "\n",
    "Question 10: Why do we have to use a batch size? Why can't we simply use all data at once? This is more relevant for even larger datasets.\n",
    "\n",
    "Question 11: How busy is the GPU for a batch size of 100? How much GPU memory is used? Hint: run 'nvidia-smi' on the cloud computer a few times during training.\n",
    "\n",
    "Question 12: What is the processing time for one training epoch when the batch size is 100? What is the processing time for one epoch when the batch size is 1,000? What is the processing time for one epoch when the batch size is 10,000? Explain the results. \n",
    "\n",
    "Question 13: How many times are the weights in the DNN updated in each training epoch if the batch size is 100? How many times are the weights in the DNN updated in each training epoch if the batch size is 1,000? How many times are the weights in the DNN updated in each training epoch if the batch size is 10,000?  \n",
    "\n",
    "Question 14: What limits how large the batch size can be?\n",
    "\n",
    "Question 15: Generally speaking, how is the learning rate related to the batch size? If the batch size is decreased, how should the learning rate be changed?\n",
    "\n",
    "Lets use a batch size of 10,000 from now on, and a learning rate of 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 14: Increasing the complexity\n",
    "\n",
    "Lets try some different configurations of number of layers and number of nodes per layer.\n",
    "\n",
    "Question 16: How many trainable parameters does the network with 4 dense layers with 50 nodes each have, compared to the initial network with 2 layers and 20 nodes per layer? Hint: use model.summary() #mark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 layers, 20 nodes, class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = ?\n",
    "\n",
    "# Build and train model\n",
    "model3 = \n",
    "\n",
    "history3 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = \n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers, 50 nodes, class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = ?\n",
    "\n",
    "# Build and train model\n",
    "model4 = \n",
    "\n",
    "history4 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = \n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 layers, 50 nodes, class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = ?\n",
    "\n",
    "# Build and train model\n",
    "model5 = \n",
    "\n",
    "history5 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = \n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 15: Batch normalization\n",
    "\n",
    "Now add batch normalization after each dense layer in `build_DNN`. Remember to import BatchNormalization from keras.layers. \n",
    "\n",
    "See https://keras.io/layers/normalization/ for information about how to call the function.\n",
    "\n",
    "Question 17: Why is batch normalization important when training deep networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers, 20 nodes, class weights, batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(123)\n",
    "\n",
    "def build_DNN(input_shape, n_layers, n_nodes, act_fun='sigmoid', optimizer='sgd', learning_rate=0.01, \n",
    "              use_bn=False, use_dropout=False, use_custom_dropout=False):\n",
    "    \n",
    "    # Setup optimizer, depending on input parameter string, put it to the last\n",
    "    opt = SGD(learning_rate=0.01)\n",
    "    # Setup a sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add layers to the model, using the input parameters of the build_DNN function\n",
    "    model.add(Input(shape=(input_shape,)  ))\n",
    "    \n",
    "    # Add first layer, requires input shape\n",
    "    model.add(Dense(n_nodes, activation=act_fun))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Add remaining layers, do not require input shape\n",
    "    for i in range(n_layers-1):    ###1\n",
    "        model.add(Dense(n_nodes, activation=act_fun))     \n",
    "        model.add(BatchNormalization())   \n",
    "       \n",
    "    # Final layer\n",
    "    model.add(Dense(1,activation='sigmoid'))  \n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer=opt,loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#mark\n",
    "# Setup some training parameters   \n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = ?\n",
    "\n",
    "# Build and train model\n",
    "model6 = \n",
    "\n",
    "history6 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = \n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 16: Activation function\n",
    "\n",
    "Try changing the activation function in each layer from sigmoid to ReLU, write down the test accuracy.\n",
    "\n",
    "Note: the last layer should still have a sigmoid activation function.\n",
    "\n",
    "https://keras.io/api/layers/activations/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers, 20 nodes, class weights, ReLU, no batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = ?\n",
    "\n",
    "# Build and train model\n",
    "model7 = \n",
    "\n",
    "history7 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = \n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 17: Optimizer\n",
    "\n",
    "Try changing the optimizer from SGD to Adam (with learning rate 0.1 as before). Remember to import the Adam optimizer from keras.optimizers. \n",
    "\n",
    "https://keras.io/optimizers/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers, 20 nodes, class weights, Adam optimizer, no batch normalization, sigmoid activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = ?\n",
    "\n",
    "# Build and train model\n",
    "model8 = \n",
    "\n",
    "history8 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = \n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 18: Dropout regularization\n",
    "\n",
    "Dropout is a type of regularization that can improve accuracy for validation and test data. \n",
    "\n",
    "Add a Dropout layer after each Dense layer (but not after the final dense layer) in `build_DNN`, with a dropout probability of 50%. Remember to first import the Dropout layer from keras.layers\n",
    "\n",
    "See https://keras.io/api/layers/regularization_layers/dropout/ for how the Dropout layer works.\n",
    "\n",
    "---\n",
    "\n",
    "Question 18: How does the validation accuracy change when adding dropout?\n",
    "\n",
    "Question 19: How does the test accuracy change when adding dropout?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://chrisalbon.com/deep_learning/keras/adding_dropout/\n",
    "from keras.layers import Dropout\n",
    "\n",
    "seed(123)\n",
    "\n",
    "def build_DNN(input_shape, n_layers, n_nodes, act_fun='sigmoid', optimizer='sgd', learning_rate=0.01, \n",
    "              use_bn=False, use_dropout=False, use_custom_dropout=False):\n",
    "    \n",
    "    # Setup optimizer, depending on input parameter string, put it to the last\n",
    "    opt = SGD(learning_rate=0.01)\n",
    "    # Setup a sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add layers to the model, using the input parameters of the build_DNN function\n",
    "    model.add(Input(shape=(input_shape,)  ))\n",
    "    \n",
    "    # Add first layer, requires input shape\n",
    "    model.add(Dense(n_nodes, activation=act_fun))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    # Add remaining layers, do not require input shape\n",
    "    for i in range(1,n_layers-1):    ###1\n",
    "        model.add(Dense(n_nodes, activation=act_fun))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.1))\n",
    "           \n",
    "       \n",
    "    # Final layer\n",
    "    model.add(Dense(1,activation='sigmoid'))  \n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer=opt,loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers, 20 nodes, class weights, dropout, SGD optimizer, no batch normalization, sigmoid activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = ?\n",
    "\n",
    "# Build and train model\n",
    "model9 = \n",
    "\n",
    "history9 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = \n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 19: Improving performance\n",
    "\n",
    "Spend some time (30 - 90 minutes) playing with the network architecture (number of layers, number of nodes per layer, activation function) and other hyper parameters (optimizer, learning rate, batch size, number of epochs, degree of regularization). For example, try a much deeper network. How much does the training time increase for a network with 10 layers?\n",
    "\n",
    "Question 20: How high classification accuracy can you achieve for the test data? What is your best configuration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find your best configuration for the DNN\n",
    "\n",
    "# Build and train DNN\n",
    "model10 = \n",
    "\n",
    "history10 = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate DNN on test data\n",
    "score = \n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 20: Dropout uncertainty\n",
    "\n",
    "Dropout can also be used during testing, to obtain an estimate of the model uncertainty. Since dropout will randomly remove connections, the network will produce different results every time the same (test) data is put into the network. This technique is called Monte Carlo dropout. For more information, see this paper http://proceedings.mlr.press/v48/gal16.pdf\n",
    "\n",
    "To achieve this, we need to redefine the Keras Dropout call by running the cell below, and use 'myDropout' in each call to Dropout, in the cell that defines the DNN. The `build_DNN` function takes two boolean arguments, use_dropout and use_custom_dropout, add a standard Dropout layer if use_dropout is true, add a myDropout layer if use_custom_dropout is true.\n",
    "\n",
    "Run the same test data through the trained network 100 times, with dropout turned on. \n",
    "\n",
    "Question 21: What is the mean and the standard deviation of the test accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import keras\n",
    "\n",
    "class myDropout(keras.layers.Dropout):\n",
    "    \"\"\"Applies Dropout to the input.\n",
    "    Dropout consists in randomly setting\n",
    "    a fraction `rate` of input units to 0 at each update during training time,\n",
    "    which helps prevent overfitting.\n",
    "    # Arguments\n",
    "        rate: float between 0 and 1. Fraction of the input units to drop.\n",
    "        noise_shape: 1D integer tensor representing the shape of the\n",
    "            binary dropout mask that will be multiplied with the input.\n",
    "            For instance, if your inputs have shape\n",
    "            `(batch_size, timesteps, features)` and\n",
    "            you want the dropout mask to be the same for all timesteps,\n",
    "            you can use `noise_shape=(batch_size, 1, features)`.\n",
    "        seed: A Python integer to use as random seed.\n",
    "    # References\n",
    "        - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](\n",
    "           http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)\n",
    "    \"\"\"\n",
    "    def __init__(self, rate, training=True, noise_shape=None, seed=None, **kwargs):\n",
    "        super(myDropout, self).__init__(rate, noise_shape=None, seed=None,**kwargs)\n",
    "        self.training = training\n",
    "\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        if 0. < self.rate < 1.:\n",
    "            noise_shape = self._get_noise_shape(inputs)\n",
    "\n",
    "            def dropped_inputs():\n",
    "                return K.dropout(inputs, self.rate, noise_shape,\n",
    "                                 seed=self.seed)\n",
    "            if not training: \n",
    "                return K.in_train_phase(dropped_inputs, inputs, training=self.training)\n",
    "            return K.in_train_phase(dropped_inputs, inputs, training=training)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your best config, custom dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your best training parameters\n",
    "\n",
    "\n",
    "# Build and train model\n",
    "model11 = \n",
    "\n",
    "history11 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell a few times to evalute the model on test data, \n",
    "# if you get slightly different test accuracy every time, Dropout during testing is working\n",
    "\n",
    "# Evaluate model on test data\n",
    "score = \n",
    "                       \n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the testing 100 times, and save the accuracies in an array\n",
    "\n",
    "    \n",
    "# Calculate and print mean and std of accuracies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 21: Cross validation uncertainty\n",
    "\n",
    "Cross validation (CV) is often used to evaluate a model, by training and testing using different subsets of the data it is possible to get the uncertainty as the standard deviation over folds. We here use a help function from scikit-learn to setup the CV, see https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html . Use 10 folds with shuffling, random state 1234. \n",
    "\n",
    "Note: We here assume that you have found the best hyper parameters, so here the data are only split into training and testing, no validation.\n",
    "\n",
    "---\n",
    "\n",
    "Question 22: What is the mean and the standard deviation of the test accuracy?\n",
    "\n",
    "Question 23: What is the main advantage of dropout compared to CV for estimating test uncertainty? The difference may not be so large in this notebook, but imagine that you have a network that takes 24 hours to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define 10-fold cross validation\n",
    "\n",
    "# Loop over cross validation folds\n",
    "    \n",
    "    # Calculate class weights for current split\n",
    "    \n",
    "    # Rebuild the DNN model, to not continue training on the previously trained model\n",
    "    \n",
    "    # Fit the model with training set and class weights for this fold\n",
    "    \n",
    "    # Evaluate the model using the test set for this fold\n",
    "    \n",
    "    # Save the test accuracy in an array\n",
    "\n",
    "# Calculate and print mean and std of accuracies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 22: DNN regression\n",
    "\n",
    "A similar DNN can be used for regression, instead of classification.\n",
    "\n",
    "Question 24: How would you change the DNN in order to use it for regression instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report\n",
    "\n",
    "Send in this jupyter notebook, with answers to all questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
